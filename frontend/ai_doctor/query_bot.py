import torch
import chromadb
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer

# 1. åµŒå…¥æ¨¡å‹ï¼ˆä¸­æ–‡é©é…ï¼‰
embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# 2. æœ¬åœ° LLM æ¨¡å‹ï¼ˆå¯æ”¹ç‚ºæ›´è¼•é‡æ¨¡å‹ï¼‰
model_id = "THUDM/chatglm3-6b"
device = torch.device("cpu")
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(device).eval()

# 3. å‘é‡è³‡æ–™åº«é€£ç·š
client = chromadb.Client()
collection = client.get_or_create_collection("menstruation_knowledge")

# 4. æå•
user_question = input("è«‹è¼¸å…¥ä½ çš„å•é¡Œï¼ˆä¸­æ–‡ï¼‰ï¼š\n> ")

# 5. æŸ¥è©¢å‘é‡åº«
query_vector = embedding_model.encode(user_question).tolist()
results = collection.query(query_embeddings=[query_vector], n_results=3)

docs = results["documents"][0]
metas = results["metadatas"][0]
context = "\n---\n".join(docs)

# é¡¯ç¤ºå¼•ç”¨è³‡æ–™èˆ‡ä¾†æº
print("\nğŸ” ä½¿ç”¨åˆ°çš„æ®µè½èˆ‡ä¾†æºï¼š\n")
for i, (doc, meta) in enumerate(zip(docs, metas), 1):
    print(f"æ®µè½ {i}: {doc[:100]}...")
    source_info = f"{meta.get('source', 'æœªçŸ¥æª”æ¡ˆ')}"
    if "page" in meta:
        source_info += f"ï¼ˆç¬¬ {meta['page']} é ï¼‰"
    print(f"ä¾†æºï¼š{source_info}\n")

# 6. å»ºç«‹ Prompt
prompt = f"""ä½ æ˜¯ä¸€ä½å…·æœ‰å©¦ç”¢ç§‘èƒŒæ™¯çš„å°ˆæ¥­ä¸­æ–‡é†«å¸«ï¼Œè«‹æ ¹æ“šä¸‹åˆ—æ®µè½å…§å®¹ï¼Œä½¿ç”¨æº«æŸ”ä¸”å…·æœ‰åŒç†å¿ƒçš„èªæ°£ï¼Œä»¥ç¹é«”ä¸­æ–‡è©³ç´°å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

ç‰¹åˆ¥æ³¨æ„äº‹é …ï¼š
1. å›ç­”å…§å®¹å¿…é ˆå¤§éƒ¨åˆ†åŸºæ–¼æä¾›çš„æ®µè½å…§å®¹ï¼Œä¸å¾—å¼•å…¥é¡å¤–æœªæåŠçš„çŸ¥è­˜æˆ–ä¸»è§€æ¨æ¸¬ã€‚
2. å¦‚æœæ®µè½ä¸­æ²’æœ‰æ˜ç¢ºæä¾›ç­”æ¡ˆï¼Œè«‹ç›´æ¥å‘ŠçŸ¥ã€Œå»ºè­°è«®è©¢å°ˆæ¥­é†«å¸«ã€ã€‚
3. èªæ°£è¦æº«æŸ”ã€æ¸…æ¥šä¸”å…·åŒç†å¿ƒï¼Œé©é‡ä½¿ç”¨å°ˆæ¥­è¡“èªã€‚
4. æœ€çµ‚å›ç­”è«‹ç°¡æ½”æ˜ç­ï¼Œé©åˆä¸€èˆ¬æ°‘çœ¾ç†è§£æˆ–å¯ä»¥è®“ä»–å€‘æ‹¿è‘—è—¥æ–¹å»è—¥å±€æˆ–æ±‚é†«ã€‚

ğŸ“š æ®µè½å…§å®¹ï¼š
{context}

ğŸ™‹ ä½¿ç”¨è€…æå•ï¼š
{user_question}

âœï¸ è«‹ä»¥ç¹é«”ä¸­æ–‡è©³ç´°ä¸”æº«æŸ”åœ°å›ç­”ï¼š

"""

# 7. ä½¿ç”¨ ChatGLM3 å›æ‡‰
response_text, _ = model.chat(tokenizer, prompt, history=[], temperature=0.7)

# 8. è¼¸å‡ºçµæœ
print("\nâœ… æœ¬åœ°æ¨¡å‹å›æ‡‰ï¼š\n")
print(response_text.strip())
